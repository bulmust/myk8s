image:
  tag: latest-dev
ingress:
  enabled: true
  ingressClassName: nginx
  hosts:
    - superset.localhost
  tls:
    - secretName: superset-tls
      hosts:
        - superset.localhost
init:
  adminUser:
    username: admin
    firstname: Superset
    lastname: Admin
    email: admin@superset.com
    password: admin
#   tolerations:
#     - key: "product"
#       operator: "Equal"
#       value: "apps"
#       effect: "NoSchedule"
#   affinity:
#     nodeAffinity:
#       requiredDuringSchedulingIgnoredDuringExecution:
#         nodeSelectorTerms:
#           - matchExpressions:
#               - key: role
#                 operator: In
#                 values:
#                   - apps
postgresql:
  enabled: true
  # auth:
  #   existingSecret:
  #   username: superset
  #   password: superset
  #   database: superset
  postgresqlPassword: superset
  primary:
    initdb:
      user: superset
      password: superset
    resources:
      limits:
        memory: 500Mi
      requests:
        cpu: 50m
        memory: 100Mi
    # affinity:
    #   nodeAffinity:
    #     requiredDuringSchedulingIgnoredDuringExecution:
    #       nodeSelectorTerms:
    #         - matchExpressions:
    #             - key: role
    #               operator: In
    #               values:
    #                 - apps
    # tolerations:
    #   - key: "product"
    #     operator: "Equal"
    #     value: "apps"
    #     effect: "NoSchedule"
    persistence:
      enabled: true
      size: 1Gi
redis:
  master:
    resources:
      limits:
        memory: 100Mi
      requests:
        cpu: 50m
        memory: 30Mi
    persistence:
      enabled: false
    # affinity:
    #   nodeAffinity:
    #     requiredDuringSchedulingIgnoredDuringExecution:
    #       nodeSelectorTerms:
    #         - matchExpressions:
    #             - key: role
    #               operator: In
    #               values:
    #                 - apps
    # tolerations:
    #   - key: "product"
    #     operator: "Equal"
    #     value: "apps"
    #     effect: "NoSchedule"
extraVolumes:
  - name: images
    emptyDir: {}
extraVolumeMounts:
  - name: images
    mountPath: /images
supersetNode:
  initContainers:
    - name: wait-for-postgres
      image: "{{ .Values.initImage.repository }}:{{ .Values.initImage.tag }}"
      imagePullPolicy: "{{ .Values.initImage.pullPolicy }}"
      envFrom:
        - secretRef:
            name: "{{ tpl .Values.envFromSecret . }}"
      command:
        - /bin/sh
        - -c
        - dockerize -wait "tcp://$DB_HOST:$DB_PORT" -timeout 120s
    # Download Logo and Favicon
    - name: icon
      image: curlimages/curl
      imagePullPolicy: "{{ .Values.initImage.pullPolicy }}"
      volumeMounts:
        - name: images
          mountPath: /images
      command:
        - /bin/sh
        - -c
        - |
          curl https://bulmust.github.io/projects/assets/myk8s.png --output /images/logo.png |
          curl https://bulmust.github.io/img/favicon.ico --output /images/favicon.ico
  command:
    - "/bin/sh"
    - "-c"
    - "cp /images/logo.png /app/superset/static/assets/images/logo.png &&\
       cp /images/favicon.ico /app/superset/static/assets/images/favicon.ico &&\
       . {{ .Values.configMountPath }}/superset_bootstrap.sh; /usr/bin/run-server.sh"
  livenessProbe:
    initialDelaySeconds: 15
    timeoutSeconds: 29
    failureThreshold: 29
    periodSeconds: 30
    successThreshold: 1
  readinessProbe:
    initialDelaySeconds: 15
    timeoutSeconds: 29
    failureThreshold: 29
    periodSeconds: 30
    successThreshold: 1
  resources:
    limits:
      memory: 3Gi
    requests:
      cpu: 50m
      memory: 1Gi
supersetWorker:
  resources:
    limits:
      memory: 2500Mi
    requests:
      cpu: 400m
      memory: 1300Mi
  livenessProbe:
    timeoutSeconds: 600
#! If you want to send email notifications, install superesetCeleryBeat
# supersetCeleryBeat:
#   enabled: true
#   resources:
#     limits:
#       memory: 600Mi
#     requests:
#       cpu: 50m
#       memory: 190Mi
configOverrides:
  A_MODULES: |
    from celery.schedules import crontab
    from flask_caching.backends.filesystemcache import FileSystemCache
  AA_MSSQL_INSTALL: |
    os.system(f"pip install pymssql")
  B_REDIS : | 
    REDIS_HOST = os.getenv("REDIS_HOST", "redis")
    REDIS_PORT = os.getenv("REDIS_PORT", "6379")
    REDIS_CELERY_DB = os.getenv("REDIS_CELERY_DB", "0")
    REDIS_RESULTS_DB = os.getenv("REDIS_RESULTS_DB", "1")
    RESULTS_BACKEND = FileSystemCache("/app/superset_home/sqllab")
  C_SECRET_KEY: |
    SECRET_KEY = 'jduidAdfj2DSak2Fcdsa23231S'
  D_DATABASE: |
    DATABASE_DIALECT = os.getenv("DATABASE_DIALECT")
    DATABASE_USER = os.getenv("DATABASE_USER")
    DATABASE_PASSWORD = os.getenv("DATABASE_PASSWORD")
    DATABASE_HOST = os.getenv("DATABASE_HOST")
    DATABASE_PORT = os.getenv("DATABASE_PORT")
    DATABASE_DB = os.getenv("DATABASE_DB")
  E_User_RELATED: |
    APP_NAME= "Bulmust"
    APP_ICON = "/static/assets/images/logo.png"  
    FAVICONS = [{"href": "/static/assets/images/favicon.ico"}]
  F_FEATURED_FLAG: |
    FEATURE_FLAGS = {
      "ALERT_REPORTS": True,
      "SCHEDULED_QUERIES": True
    }
  # G_EMAIL_NOTIFICATIONS: |
  #   EMAIL_NOTIFICATIONS = True  # all the emails are sent using dryrun
  #   SMTP_HOST = "outlook.office365.com" # change to your host
  #   SMTP_PORT = 25 # your port, e.g. 587
  #   SMTP_STARTTLS = True
  #   SMTP_SSL_SERVER_AUTH = False # If your using an SMTP server with a valid certificate
  #   SMTP_SSL = False
  #   SMTP_USER = "bulmust2@gmail.com" # use the empty string "" if using an unauthenticated SMTP server
  #   SMTP_PASSWORD = "smtp-password" # use the empty string "" if using an unauthenticated SMTP server
  #   SMTP_MAIL_FROM = "bulmust2@gmail.com"
  #   EMAIL_REPORTS_SUBJECT_PREFIX = "[EMAIL SUBJECT] " # optional - overwrites default value in config.py of "[Report] "
  #   ENABLE_CHUNK_ENCODING = False
  #   ALERT_REPORTS_NOTIFICATION_DRY_RUN = False
  #   WEBDRIVER_BASEURL = "https://.com"
  #   # The base URL for the email report hyperlinks.
  #   WEBDRIVER_BASEURL_USER_FRIENDLY = WEBDRIVER_BASEURL
  #   SQLLAB_CTAS_NO_LIMIT = True
  H_SCHEDULE_QUERY: |
    SCHEDULED_QUERIES = {
        # This information is collected when the user clicks "Schedule query",
        # and saved into the `extra` field of saved queries.
        # See: https://github.com/mozilla-services/react-jsonschema-form
        'JSONSCHEMA': {
            'title': 'Schedule',
            'description': (
                'In order to schedule a query, you need to specify when it '
                'should start running, when it should stop running, and how '
                'often it should run. You can also optionally specify '
                'dependencies that should be met before the query is '
                'executed. Please read the documentation for best practices '
                'and more information on how to specify dependencies.'
            ),
            'type': 'object',
            'properties': {
                'output_table': {
                    'type': 'string',
                    'title': 'Output table name',
                },
                'start_date': {
                    'type': 'string',
                    'title': 'Start date',
                    # date-time is parsed using the chrono library, see
                    # https://www.npmjs.com/package/chrono-node#usage
                    'format': 'date-time',
                    'default': 'tomorrow at 9am',
                },
                'end_date': {
                    'type': 'string',
                    'title': 'End date',
                    # date-time is parsed using the chrono library, see
                    # https://www.npmjs.com/package/chrono-node#usage
                    'format': 'date-time',
                    'default': '9am in 30 days',
                },
                'schedule_interval': {
                    'type': 'string',
                    'title': 'Schedule interval',
                },
                'dependencies': {
                    'type': 'array',
                    'title': 'Dependencies',
                    'items': {
                        'type': 'string',
                    },
                },
            },
        },
        'UISCHEMA': {
            'schedule_interval': {
                'ui:placeholder': '@daily, @weekly, etc.',
            },
            'dependencies': {
                'ui:help': (
                    'Check the documentation for the correct format when '
                    'defining dependencies.'
                ),
            },
        },
        'VALIDATION': [
            # ensure that start_date <= end_date
            {
                'name': 'less_equal',
                'arguments': ['start_date', 'end_date'],
                'message': 'End date cannot be before start date',
                # this is where the error message is shown
                'container': 'end_date',
            },
        ],
        # link to the scheduler; this example links to an Airflow pipeline
        # that uses the query id and the output table as its name
        'linkback': (
            'https://airflow.example.com/admin/airflow/tree?'
            'dag_id=query_${id}_${extra_json.schedule_info.output_table}'
        ),
    }
  J_CACHE_CONFIG: |
    CACHE_CONFIG = {
        "CACHE_TYPE": "RedisCache",
        "CACHE_DEFAULT_TIMEOUT": 3600,
        "CACHE_KEY_PREFIX": "superset_",
        "CACHE_REDIS_HOST": REDIS_HOST,
        "CACHE_REDIS_PORT": REDIS_PORT,
        "CACHE_REDIS_DB": REDIS_RESULTS_DB,
    }
    DATA_CACHE_CONFIG = CACHE_CONFIG
    class CeleryConfig:
        broker_url = f"redis://{REDIS_HOST}:{REDIS_PORT}/0"

        imports = (
            "superset.sql_lab",
            "superset.tasks.scheduler",
        )

        result_backend = f"redis://{REDIS_HOST}:{REDIS_PORT}/0"
        worker_prefetch_multiplier = 10
        task_acks_late = True
        task_annotations = {
            "sql_lab.get_sql_results": {
                "rate_limit": "100/s",
            },
        }
        beat_schedule = {
            "reports.scheduler": {
                "task": "reports.scheduler",
                "schedule": crontab(minute="*", hour="*"),
            },
            "reports.prune_log": {
                "task": "reports.prune_log",
                "schedule": crontab(minute=0, hour=0),
            },
        }
    CELERY_CONFIG = CeleryConfig
  K_SQLALCHEMY: |
    SQLALCHEMY_POOL_TIMEOUT = 600
    SUPERSET_WEBSERVER_TIMEOUT = 600
    SQLLAB_TIMEOUT = 600
    SCREENSHOT_LOCATE_WAIT = 600
    SCREENSHOT_LOAD_WAIT = 600
# tolerations:
#   - key: "product"
#     operator: "Equal"
#     value: "apps"
#     effect: "NoSchedule"
# affinity:
#   nodeAffinity:
#     requiredDuringSchedulingIgnoredDuringExecution:
#       nodeSelectorTerms:
#         - matchExpressions:
#             - key: role
#               operator: In
#               values:
#                 - apps